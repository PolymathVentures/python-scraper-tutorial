# Crear un scraper en Python en menos de 10 minutos

Algunas p√°ginas pueden tener informaci√≥n que es valiosa para nuestro negocio o para entender mejor un mercado, normalmente si queremos extraer la informaci√≥n de estas p√°ginas nuestra primera opci√≥n ser√≠a realizar un proceso manual engorroso que consumir√≠a mucho tiempo y esfuerzo de una o varias personas, afortunadamente para estos casos los scripts de programaci√≥n con python pueden ayudarnos a no solo ahorrar tiempo si no hacer los procesos de forma autom√°tica o masiva.

## ¬øQu√© es un scraper y en qu√© casos lo necesito?
Es el proceso por el cual de forma autom√°tica minamos, extraemos y almacenamos datos de una p√°gina web por el protocolo http, es decir, un web scraper es un programa que utilizamos para obtener grandes cantidades de datos de una p√°gina web de una forma automatizada, es como si un robot leyera toda la informaci√≥n y extrae la que le indiquemos

Para poder hacer web scraping se deben realizar dos procesos:
 - Fetching: obtener o descargar el contenido de la p√°gina por el protocolo http
 - Extracting: Extraer los datos que queremos de la p√°gina

## ¬øQu√© vamos a construir?
En este tutorial de PolyMathDevs vamos a ver c√≥mo desarrollar un web scraper con python sobre la pagina web https://www.imdb.com/chart/top/  en la cual nos muestra una lista de las 250 mejores pel√≠culas

![Screenshot](website.png)

Una vez desarrollemos nuestro script tomar√° la informaci√≥n del nombre de la pelicula, a√±o de lanzamiento y calificaci√≥n y nos guardar√° esta informaci√≥n en un csv c√≥mo se puede ver a continuaci√≥n:

![Screenshot](csv.png)

## Contrucci√≥n del scraper:
Para poder realizar el scraper lo primero que vamos a realizar es obtener el contenido de la p√°gina web que estamos realizando el scraper en nuestro caso es https://www.imdb.com/chart/top/, esto lo realizamos por medio de la librer√≠a request de python y para poder manejar el contenido de la p√°gina de una forma mas clara utilizaremos la librer√≠a BeautifulSoup.

- Requests: Liberia HTTP de python para hacer http request mas faciles
- BeautifulSoup: M√≥dulo de la librer√≠a BS4 de python que analiza la pagina y extrae la informaci√≥n que le indicamos y la formatea para que sea m√°s f√°cil de manipular

Aunque python por defecto tiene la posibilidad de formatear el contenido html utilizamos BeautifulSoup por que nos brinda un mejor formato, dentro de las ventajas que obtenemos estan:

- Velocidad
- Formatea la pagina igual que un navegador
- Embellece o estandariza mejor el c√≥digo fuente

Debemos instalar las librer√≠as
```bash
  pip install requests bs4
```

Si no lo puedes realizar con pip puedes validar con pip3

Para validar que est√© bien creamos un nuevo archivo scraper.py donde importamos las librerias
```python
import requests ,bs4
from bs4 import BeautifulSoup
```

Ejecutamos el script scraper.py 
```bash
  python scraper.py
```

Si no lo puedes realizar con python puedes validar con python3 scraper.py 

![Screenshot](goodrun.jpg)

Si al ejecutar el script no obtenemos ning√∫n error significa que tenemos bien instaladas las librer√≠as üëç, ahora obtenemos la p√°gina web con request agregando las siguientes lineas a nuestro archivo scraper.py 
```python
url = 'https://www.imdb.com/chart/top/'
response = requests.get(url)
```

Como ya tenemos nuestra pagina web ahora vamos a utilizar BeautifulSoup para poder obtener directamente cualquier tag o elemento html que deseemos, para esto agregamos la siguiente linea nuestro script scraper.py 
```python
root =  BeautifulSoup(response.content, 'html.parser')
```



Ya con nuestra p√°gina en nuestro objeto root formateada podemos navegar por el contenido html de la p√°gina que obtuvimos con requests y extraer la informaci√≥n, antes de hacer esto veamos algunos ejemplos de c√≥mo podemos obtener informaci√≥n con BeautifulSoup

Podemos extraer contenido de BeautifulSoup con los siguientes comandos:

| Acci√≥n | Codigo  |
| ------- | --- |
| Obtener el t√≠tulo de la p√°gina el elemento <title> | root.title |
| Obtener el body de la p√°gina el elemento <body> | root.body |
| Obtener el primer div que se encuentra en el body | root.body.div |
| Obtener todos los div‚Äôs que est√°n en la p√°gina | root.find_all('div') |
| Obtener todos los div‚Äôs que est√°n en la p√°gina con una clase* espec√≠fica | root.find_all('div', class_='subnav_item_main') |
| Obtener el div‚Äôs que est√°n en la p√°gina con un id especifico | root.find_all('div', id='success-story-929') |
| Obtener todos los elementos que una clase | root.select( '.subnav_item_main') |
| Obtener un elemento con una clase especifica | root.select_one('.success-story a') |

> *En python la palabra class es reservada es por esto que en este m√©todo debemos pasarle el identificador class_ si quisi√©ramos utilizar la palabra reservada class debemos pasarla como un atributo como se muestra a continuaci√≥n
```python
root.find_all('div', {'class':success-story-item})
```

Es importante tener presente que el contenido que retorna BeautifulSoup no es de tipo text si no de tipo tag , estamos obteniendo directamente todo el elemento tag del html
```python
print(type(root.body.div))
```
Si queremos obtener el contenido dentro del tag html debemos utilizar el m√©todo text
```python
print(root.body.div.text)
```

Teniendo lo anterior claro ahora vamos ha realizar el scraper, para esto lo primero es definir qu√© datos queremos obtener es diferente si queremos traer todo un objeto o s√≥lo cierta informaci√≥n, para nuestro ejemplo vamos a obtener el t√≠tulo , la fecha de estreno y el rating de cada una de las 250 pel√≠culas que est√°n en la lista

Para esto vamos a utilizar el inspector de c√≥digo sobre la p√°gina para validar que informaci√≥n tienen en com√∫n los bloques o c√≥mo est√°n construidos para poder identificar qu√© par√°metro es sobre el cual vamos a realizar la b√∫squeda

![Screenshot](inspect1.jpg)

Como podemos ver en la imagen todas las listas est√°n en una tabla con la clase ‚Äúlister-list‚Äù y dentro de este tabla cada pel√≠cula ocupa una fila , lo primero que realizaremos es obtener el body de la tabla con la clase ‚Äúlister-list‚Äù y despues obtenemos todas las filas del tbody, para esto agregamos la siguiente l√≠nea a nuestro script
```python
rows = root.select("tbody.lister-list tr")
```

Ahora que ya tenemos la tabla podemos obtener cada fila de la pelicula realizando un loop sobre los elementos de la tabla
```python
for movie in rows
	print(movie)
```

Si vemos el resultado de ejecutar nuestros script encontramos que ya tenemos el elemento tr , ahora debemos ir al detalle de cada celda para obtener:

![Screenshot](output.jpg)
### El nombre:
![Screenshot](inspect2.jpg)

El nombre se encuentra dentro de la fila tr en una celda td con la clase ‚ÄútitleColum‚Äù y dento de un tag de enlace <a> , para poder obtenerlo agregamos la siguiente linea
```python
title = movie.select_one(".titleColumn a").text
```

### El a√±o de lanzamiento:
![Screenshot](inspect3.jpg)

El a√±o de lanzamiento se encuentra dentro de la misma celda td que el nombre, solo que se encuentra dentro de un tag <span>
```python
year = movie.select_one(".titleColumn span").text
```

### La Calificaci√≥n(Rating):
![Screenshot](inspect4.jpg)

La Calificaci√≥n se encuentra dentro de la celda td con la clase ‚ÄúimdbRating‚Äù y dentro de un tag <strong>
```python
rating = movie.select_one(".ratingColumn strong").text
```


Dando por resultado final que nuestros script sea de la siguiente forma:
```python
import requests ,bs4
from bs4 import BeautifulSoup

url = 'https://www.imdb.com/chart/top/'
response = requests.get(url)
root = BeautifulSoup(response.content, "html.parser")
rows = root.select("tbody.lister-list tr")

for movie in rows:
	title = movie.select_one(".titleColumn a").text
	year = movie.select_one(".titleColumn span").text
	rating = movie.select_one(".ratingColumn strong").text
```
	
## Guardando los datos en un archivo csv:
Ya que logramos tener toda la informaci√≥n de las pel√≠culas que deseamos ahora vamos a almacenarla en un archivo csv, para esto vamos a importar la librer√≠a csv
```python
import csv
```
Vamos a abrir el documento csv donde vamos a guardar nuestras pel√≠culas(movies.csv) y vamos a crear un writer para poder escribir en el  
```python
f = open('movies.csv', 'w')
writer = csv.writer(f)
```

Primero agregamos los headers
```python
header = ['Title', 'ReleaseDate', 'Rating']
writer.writerow(header)
```

Y vamos a guardar la informaci√≥n de cada pel√≠cula mientras recorremos la lista de objetos writer.writerow([title,year,rating])

Por √∫ltimo cerramos el documento movies.csv
```python
f.close()
```

Para garantizar que siempre se ejecuto el codigo podemos remplazar la apertura y el cierre del archivo:
```python
f = open('movies.csv', 'w')
.......
f.close()
```
por: 
```python
with open("movies.csv", "w") as f:
	.......
```

Y nuestro c√≥digo completo quedar√≠a de la siguiente manera:
```python
import requests ,bs4
from bs4 import BeautifulSoup
import csv

url = 'https://www.imdb.com/chart/top/'
response = requests.get(url)

root = BeautifulSoup(response.content, "html.parser")
rows = root.select("tbody.lister-list tr")

with open("movies.csv", "w") as f:

    writer = csv.writer(f)
    writer.writerow(header)

    for movie in rows:
        title = movie.select_one(".titleColumn a").text
        year = movie.select_one(".titleColumn span").text
        rating = movie.select_one(".ratingColumn strong").text

        writer.writerow([title, year, rating])
```


**Importante:** es necesario que cuando realices un scraping tengas presente las pol√≠ticas de la p√°gina que deseas scrapear para estar seguro que no se est√° infringiendo ninguna ley, esta validaci√≥n la puedes realizar tambi√©n ingresando al dominio principal del sitio y agregando /robots.txt donde puedes validar si la url sobre la que est√°s realizando el scraper est√° autorizada o no , para nuestro ejemplo vamos a utilizar la p√°gina de imdb s√≥lo con fines educativos

## Conclusiones finales
Con unas pocas l√≠neas de c√≥digo es posible obtener la informaci√≥n de alguna p√°gina de forma ordenada y autom√°tica, esto nos puede funcionar para hacer una migraci√≥n masiva de datos de un sitio a otro , para estudios de mercado o para tener informaci√≥n confiable en tiempo real, si te gusto este contenido y quieres que te notifiqumos cuando publiquemos algo nuevo inscribete en nuestro newsletter  [aqu√≠](https://polymathv.us3.list-manage.com/subscribe/post?u=bf424acd364ec846df798dafc&id=59ef62c1d3).

